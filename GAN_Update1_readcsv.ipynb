{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory created at GAN-Output\n"
     ]
    }
   ],
   "source": [
    "project_path = 'GAN-Output'\n",
    "if not os.path.exists(project_path):\n",
    "    os.makedirs(project_path)\n",
    "print(f\"Project directory created at {project_path}\")\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_path = r'Urdu_Dataset\\urdu_batch_1.csv'  # Update this with the actual path to your CSV file\n",
    "data_path = r'GAN-Output\\urdu_text.txt'  # Update this with the desired path to save the text file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\faiza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# First, ensure we have the device set up\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully at: GAN-Output\\urdu_text.txt\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess and combine text data from the CSV\\\n",
    "def load_and_process_data(csv_path):\n",
    "    try:\n",
    "        # Load the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Ensure the relevant column containing the text data is named appropriately\n",
    "        if 'News Text' not in df.columns:\n",
    "            raise ValueError(\"Expected column 'News Text' not found in the CSV file.\")\n",
    "\n",
    "        # Preprocess and combine all text data into a single string\n",
    "        all_text = ' '.join(df['News Text'].dropna().tolist())\n",
    "\n",
    "        return all_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing the CSV file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load and process data from the CSV file\n",
    "article_text = load_and_process_data(csv_path)\n",
    "\n",
    "# Save the processed data to a text file\n",
    "if article_text:\n",
    "    try:\n",
    "        with open(data_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(article_text)\n",
    "        print(f\"Data saved successfully at: {data_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving the data to file: {str(e)}\")\n",
    "else:\n",
    "    print(\"No data to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 7648\n",
      "Vocabulary size: 828\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Remove non-Urdu characters while keeping necessary punctuation\n",
    "    urdu_pattern = r'[\\u0600-\\u06FF\\s\\.\\،\\؟]+'\n",
    "    cleaned = re.findall(urdu_pattern, text)\n",
    "    cleaned_text = ' '.join(cleaned)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    # Remove very short lines\n",
    "    cleaned_text = '\\n'.join(line for line in cleaned_text.split('\\n')\n",
    "                            if len(line.strip()) > 10)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Load and clean text\n",
    "with open(data_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "cleaned_text = clean_text(text)\n",
    "tokens = word_tokenize(cleaned_text)\n",
    "\n",
    "# Create vocabulary\n",
    "special_tokens = ['<PAD>', '<UNK>', '<START>', '<END>']\n",
    "word2id = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "\n",
    "# Count word frequencies\n",
    "vocab = Counter(tokens)\n",
    "min_freq = 2  # Minimum frequency threshold\n",
    "\n",
    "# Add frequent words to vocabulary\n",
    "idx = len(special_tokens)\n",
    "for word, count in vocab.most_common():\n",
    "    if count >= min_freq:\n",
    "        word2id[word] = idx\n",
    "        idx += 1\n",
    "\n",
    "id2word = {idx: word for word, idx in word2id.items()}\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 764\n",
      "Number of batches: 95\n"
     ]
    }
   ],
   "source": [
    "class UrduDataset(Dataset):\n",
    "    def __init__(self, tokens, word2id, seq_length):\n",
    "        self.tokens = tokens\n",
    "        self.word2id = word2id\n",
    "        self.seq_length = seq_length\n",
    "        self.sequences = self._create_sequences()\n",
    "\n",
    "        if len(self.sequences) == 0:\n",
    "            raise ValueError(\"No sequences were created. Check your data and sequence length.\")\n",
    "\n",
    "    def _create_sequences(self):\n",
    "        sequences = []\n",
    "        for i in range(0, len(self.tokens) - self.seq_length, self.seq_length):\n",
    "            seq = self.tokens[i:i + self.seq_length + 1]\n",
    "            if len(seq) == self.seq_length + 1:\n",
    "                seq_ids = [self.word2id.get(word, self.word2id['<UNK>'])\n",
    "                          for word in seq]\n",
    "                sequences.append(seq_ids)\n",
    "        return sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        return (torch.tensor(sequence[:-1], dtype=torch.long),\n",
    "                torch.tensor(sequence[1:], dtype=torch.long))\n",
    "\n",
    "# Create dataset and dataloader\n",
    "seq_length = 10  # Adjusted for small dataset\n",
    "batch_size = 8   # Adjusted for small dataset\n",
    "\n",
    "dataset = UrduDataset(tokens, word2id, seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Core layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=4)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim * 4)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.output_layer = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "\n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim * 2)\n",
    "\n",
    "        # Activation\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "\n",
    "        # Self-attention\n",
    "        attn_output, _ = self.attention(\n",
    "            lstm_out.transpose(0, 1),\n",
    "            lstm_out.transpose(0, 1),\n",
    "            lstm_out.transpose(0, 1)\n",
    "        )\n",
    "        attn_output = attn_output.transpose(0, 1)\n",
    "\n",
    "        # First residual + norm\n",
    "        output = self.layer_norm1(lstm_out + attn_output)\n",
    "\n",
    "        # Fully connected with residual\n",
    "        residual = output\n",
    "        output = self.fc1(output)\n",
    "        output = self.gelu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        # Second residual + norm\n",
    "        output = self.layer_norm2(output + residual)\n",
    "\n",
    "        # Output\n",
    "        logits = self.output_layer(output)\n",
    "\n",
    "        return logits, None\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=8)\n",
    "\n",
    "        # Fully connected layers with increased capacity\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim * 4)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.output_layer = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "\n",
    "        # Regularization and normalization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim * 2)\n",
    "\n",
    "        # Activation functions\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "\n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "\n",
    "        # Self-attention mechanism\n",
    "        attn_output, _ = self.attention(\n",
    "            lstm_out.transpose(0, 1),\n",
    "            lstm_out.transpose(0, 1),\n",
    "            lstm_out.transpose(0, 1)\n",
    "        )\n",
    "        attn_output = attn_output.transpose(0, 1)\n",
    "\n",
    "        # First residual connection and layer norm\n",
    "        output = self.layer_norm1(lstm_out + attn_output)\n",
    "\n",
    "        # Fully connected layers with residual connection\n",
    "        residual = output\n",
    "        output = self.fc1(output)\n",
    "        output = self.gelu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        # Second residual connection and layer norm\n",
    "        output = self.layer_norm2(output + residual)\n",
    "\n",
    "        # Final output layer\n",
    "        logits = self.output_layer(output)\n",
    "\n",
    "        return logits, None\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Core layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Attention\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=4)\n",
    "\n",
    "        # Fully connected\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim * 4)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Activation\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "\n",
    "        # Self-attention\n",
    "        attn_output, _ = self.attention(\n",
    "            lstm_out.transpose(0, 1),\n",
    "            lstm_out.transpose(0, 1),\n",
    "            lstm_out.transpose(0, 1)\n",
    "        )\n",
    "        attn_output = attn_output.transpose(0, 1)\n",
    "\n",
    "        # Residual + norm\n",
    "        output = self.layer_norm1(lstm_out + attn_output)\n",
    "\n",
    "        # Global average pooling\n",
    "        output = torch.mean(output, dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        output = self.fc1(output)\n",
    "        output = self.gelu(output)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        output = self.fc2(output)\n",
    "        output = self.layer_norm2(output)\n",
    "        output = self.gelu(output)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # Final classification\n",
    "        output = self.fc3(output)\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        smoothed_target = torch.zeros_like(pred)\n",
    "        smoothed_target.fill_(self.smoothing / (pred.size(-1) - 1))\n",
    "        smoothed_target.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n",
    "        return self.criterion(pred.log_softmax(dim=-1), smoothed_target)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=40, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            return False\n",
    "\n",
    "        if val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "def add_noise_to_inputs(tensor, noise_factor=0.1):\n",
    "    noise = torch.randn_like(tensor) * noise_factor\n",
    "    return tensor + noise\n",
    "\n",
    "def get_warmup_schedule(optimizer, num_warmup_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return 1.0\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "# Optimizers\n",
    "g_optimizer = torch.optim.AdamW(\n",
    "    generator.parameters(),\n",
    "    lr=0.0001,\n",
    "    betas=(0.5, 0.999),\n",
    "    weight_decay=0.001\n",
    ")\n",
    "\n",
    "d_optimizer = torch.optim.AdamW(\n",
    "    discriminator.parameters(),\n",
    "    lr=0.0001,\n",
    "    betas=(0.5, 0.999),\n",
    "    weight_decay=0.001\n",
    ")\n",
    "\n",
    "# Schedulers\n",
    "g_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    g_optimizer,\n",
    "    mode='min',\n",
    "    factor=0.7,\n",
    "    patience=10,\n",
    "    \n",
    ")\n",
    "\n",
    "d_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    d_optimizer,\n",
    "    mode='min',\n",
    "    factor=0.7,\n",
    "    patience=10,\n",
    "\n",
    ")\n",
    "\n",
    "# Loss functions\n",
    "adversarial_criterion = nn.BCELoss()\n",
    "content_criterion = LabelSmoothingLoss(smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, device):\n",
    "    batch_size = real_samples.size(0)\n",
    "    seq_len = real_samples.size(1)\n",
    "\n",
    "    # Create random weight for interpolation\n",
    "    alpha = torch.rand(batch_size, 1).expand(-1, seq_len).to(device)\n",
    "\n",
    "    # Convert to float for interpolation\n",
    "    real_float = real_samples.float()\n",
    "    fake_float = fake_samples.float()\n",
    "\n",
    "    # Interpolate between real and fake samples\n",
    "    interpolates = (alpha * real_float + ((1 - alpha) * fake_float))\n",
    "\n",
    "    # Create embeddings for interpolated samples\n",
    "    with torch.no_grad():\n",
    "        discrete_interpolates = torch.clamp(interpolates.round(), min=0, max=vocab_size-1).long()\n",
    "        embedded_interpolates = discriminator.embedding(discrete_interpolates)\n",
    "\n",
    "    embedded_interpolates.requires_grad_(True)\n",
    "\n",
    "    # Disable CuDNN for gradient computation\n",
    "    with torch.backends.cudnn.flags(enabled=False):\n",
    "        # Pass embedded interpolates through the discriminator components\n",
    "        embedded = discriminator.dropout(embedded_interpolates)\n",
    "        output, _ = discriminator.lstm(embedded)\n",
    "        output = torch.mean(output, dim=1)\n",
    "        output = discriminator.fc1(output)\n",
    "        output = F.relu(output)\n",
    "        output = discriminator.dropout(output)\n",
    "        d_interpolates = discriminator.fc2(output)\n",
    "        d_interpolates = torch.sigmoid(d_interpolates)\n",
    "\n",
    "    # Calculate gradients\n",
    "    grad_outputs = torch.ones_like(d_interpolates).to(device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=embedded_interpolates,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "\n",
    "    # Calculate gradient penalty\n",
    "    gradients = gradients.reshape(batch_size, -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "    return gradient_penalty\n",
    "\n",
    "    # Define optimizers and loss functions\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "adversarial_criterion = nn.BCELoss()\n",
    "content_criterion = nn.CrossEntropyLoss(ignore_index=word2id['<PAD>'])\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=0.001):  # Increased patience and added min_delta\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            return False\n",
    "\n",
    "        if val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return False\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, num_epochs, device, gradient_accumulation_steps=4):\n",
    "    early_stopping = EarlyStopping(patience=40, min_delta=0.001)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Warmup\n",
    "    warmup_steps = 100\n",
    "    warmup_scheduler_g = get_warmup_schedule(g_optimizer, warmup_steps)\n",
    "    warmup_scheduler_d = get_warmup_schedule(d_optimizer, warmup_steps)\n",
    "\n",
    "    # Track best losses\n",
    "    best_g_loss = float('inf')\n",
    "    best_d_loss = float('inf')\n",
    "\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc='Training Progress')\n",
    "    print(\"Starting training... inside train gan\")\n",
    "    for epoch in epoch_pbar:\n",
    "        total_d_loss = 0\n",
    "        total_g_loss = 0\n",
    "        batch_count = 0\n",
    "        print(\"Inside loop now\")\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        print(\"After functions\")\n",
    "        batch_pbar = tqdm(dataloader, leave=False, desc=f'Epoch {epoch}')\n",
    "\n",
    "        for i, (real_seq, target_seq) in enumerate(batch_pbar):\n",
    "            batch_size = real_seq.size(0)\n",
    "            real_seq = real_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "            print(\"Inside inner loop\")\n",
    "            # Train Discriminator\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            # Real samples with noise\n",
    "            noisy_real_seq = add_noise_to_inputs(real_seq.float(), noise_factor=0.1).long()\n",
    "            real_outputs = discriminator(noisy_real_seq)\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            d_loss_real = adversarial_criterion(real_outputs, real_labels)\n",
    "\n",
    "            # Fake samples with noise\n",
    "            with torch.no_grad():\n",
    "                noise = torch.randint(0, vocab_size, (batch_size, real_seq.size(1)), device=device)\n",
    "                fake_seq, _ = generator(noise)\n",
    "                fake_tokens = fake_seq.argmax(dim=-1)\n",
    "\n",
    "            noisy_fake_tokens = add_noise_to_inputs(fake_tokens.float(), noise_factor=0.1).long()\n",
    "            fake_outputs = discriminator(noisy_fake_tokens)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            d_loss_fake = adversarial_criterion(fake_outputs, fake_labels)\n",
    "\n",
    "            # Total discriminator loss with gradient accumulation\n",
    "            d_loss = (d_loss_real + d_loss_fake) / gradient_accumulation_steps\n",
    "            d_loss.backward()\n",
    "\n",
    "            if (i + 1) % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "                d_optimizer.step()\n",
    "                discriminator.zero_grad()\n",
    "\n",
    "            # Train Generator\n",
    "            generator.zero_grad()\n",
    "\n",
    "            # Generate new fake samples\n",
    "            fake_seq, _ = generator(noise)\n",
    "            fake_tokens = fake_seq.argmax(dim=-1)\n",
    "            fake_outputs = discriminator(fake_tokens)\n",
    "\n",
    "            # Generator losses with gradient accumulation\n",
    "            g_loss_adv = adversarial_criterion(fake_outputs, real_labels)\n",
    "            g_loss_content = content_criterion(fake_seq.transpose(1, 2), target_seq)\n",
    "            g_loss = (0.5 * g_loss_adv + 0.5 * g_loss_content) / gradient_accumulation_steps\n",
    "\n",
    "            g_loss.backward()\n",
    "\n",
    "            if (i + 1) % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "                g_optimizer.step()\n",
    "                generator.zero_grad()\n",
    "\n",
    "            # Update tracking\n",
    "            total_d_loss += d_loss.item() * gradient_accumulation_steps\n",
    "            total_g_loss += g_loss.item() * gradient_accumulation_steps\n",
    "            batch_count += 1\n",
    "\n",
    "            batch_pbar.set_postfix({\n",
    "                'D_loss': f'{d_loss.item():.4f}',\n",
    "                'G_loss': f'{g_loss.item():.4f}'\n",
    "            })\n",
    "        print(\"After inner loop\")\n",
    "        # Calculate average losses\n",
    "        avg_d_loss = total_d_loss / batch_count\n",
    "        avg_g_loss = total_g_loss / batch_count\n",
    "\n",
    "        # Update learning rates\n",
    "        if epoch < warmup_steps:\n",
    "            warmup_scheduler_g.step()\n",
    "            warmup_scheduler_d.step()\n",
    "        else:\n",
    "            g_scheduler.step(avg_g_loss)\n",
    "            d_scheduler.step(avg_d_loss)\n",
    "\n",
    "        # Save best models\n",
    "        if avg_g_loss < best_g_loss:\n",
    "            best_g_loss = avg_g_loss\n",
    "            torch.save(generator.state_dict(), 'best_generator.pt')\n",
    "        if avg_d_loss < best_d_loss:\n",
    "            best_d_loss = avg_d_loss\n",
    "            torch.save(discriminator.state_dict(), 'best_discriminator.pt')\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopping(avg_g_loss):\n",
    "            print(\"\\nEarly stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  95%|█████████▌| 19/20 [27:11<01:33, 93.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After inner loop\n",
      "EarlyStopping counter: 14 out of 40\n",
      "Inside loop now\n",
      "After functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside inner loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 20/20 [28:25<00:00, 85.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After inner loop\n",
      "EarlyStopping counter: 15 out of 40\n",
      "Training completed successfully!\n",
      "Models saved as 'best_generator.pt' and 'best_discriminator.pt'\n"
     ]
    }
   ],
   "source": [
    "# Training execution with adjusted parameters\n",
    "try:\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "    # Adjusted training parameters\n",
    "    num_epochs = 20  # Increased from 100\n",
    "    batch_size = 25   # Increased from 8\n",
    "\n",
    "    # Create new dataloader with adjusted batch size\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    print(\"1st Step\")\n",
    "    # Initialize early stopping with new parameters\n",
    "    early_stopping = EarlyStopping(patience=15, min_delta=0.001)\n",
    "    print(\"2nd Step\")\n",
    "    # Train the models\n",
    "    generator, discriminator = train_gan(generator, discriminator, dataloader, num_epochs, device)\n",
    "    print(\"Training completed successfully!\")\n",
    "    print(\"Models saved as 'best_generator.pt' and 'best_discriminator.pt'\")\n",
    "    # Save trained model\n",
    "    torch.save({\n",
    "        'generator_state': generator.state_dict(),\n",
    "        'discriminator_state': discriminator.state_dict(),\n",
    "        'g_optimizer_state': g_optimizer.state_dict(),\n",
    "        'd_optimizer_state': d_optimizer.state_dict(),\n",
    "    }, os.path.join(project_path, 'trained_model.pt'))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text samples with different temperatures...\n",
      "\n",
      "Temperature: 0.5\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 1:\n",
      "<UNK> <UNK> توقع کا <UNK> <UNK> <UNK> <UNK> <UNK> کی کا <UNK> نے <UNK> کے <UNK> کی <UNK> کی کے\n",
      "\n",
      "Sample 2:\n",
      "کا <UNK> <UNK> <UNK> <UNK> <UNK> میں <UNK> ادائیگی <UNK> کی <UNK> <UNK> کے کی <UNK> سے <UNK> <UNK> <UNK>\n",
      "\n",
      "Sample 3:\n",
      "<UNK> کے <UNK> <UNK> کی میں <UNK> میں ارب کے کے <UNK> <UNK> <UNK> کی <UNK> <UNK> کی <UNK> <UNK>\n",
      "\n",
      "Temperature: 0.7\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 1:\n",
      "کا <UNK> لیے <UNK> <UNK> <UNK> کر میں کی میں <UNK> <UNK> کی میں سے بعد <UNK> کی سے کہا\n",
      "\n",
      "Sample 2:\n",
      "سے <UNK> کی <UNK> کی کی میں <UNK> نے پر کی <UNK> کی میں نے نے <UNK> <UNK> میں <UNK>\n",
      "\n",
      "Sample 3:\n",
      "<UNK> عثمانی کی میں میں ڈالر <UNK> میں کے <UNK> <UNK> بینک <UNK> نے ڈی کی کے فیصد <UNK> <UNK>\n",
      "\n",
      "Temperature: 1.0\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 1:\n",
      "مزید پاور <UNK> کی کی متعارف کی سے میں <UNK> <UNK> کی کہ کے تھا سطح <UNK> فائل فنڈز سے\n",
      "\n",
      "Sample 2:\n",
      "یعنی ارب نے یا میں اضافہ نے <UNK> گروپ سے جاری محفوظ کی نے اور ہیںمزید کے کی کروانے دینے\n",
      "\n",
      "Sample 3:\n",
      "پاک تقریبا <UNK> کہ ان سال نہیں باعث کے کے ۱۲۹ دیگر مطابق کے سامان ڈی میڈیا اسمارٹ <UNK> دو\n"
     ]
    }
   ],
   "source": [
    "def generate_text(generator, word2id, id2word, seq_length=10, num_samples=5, temperature=1.0, device='cuda'):\n",
    "    generator.eval()\n",
    "    generated_texts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            current_sequence = [word2id['<START>']]\n",
    "\n",
    "            for _ in range(seq_length):\n",
    "                sequence_tensor = torch.LongTensor([current_sequence]).to(device)\n",
    "                output, _ = generator(sequence_tensor)\n",
    "\n",
    "                next_token_logits = output[0, -1, :] / temperature\n",
    "                next_token_probs = F.softmax(next_token_logits, dim=0)\n",
    "                next_token_id = torch.multinomial(next_token_probs, 1).item()\n",
    "\n",
    "                current_sequence.append(next_token_id)\n",
    "\n",
    "                if next_token_id == word2id['<END>']:\n",
    "                    break\n",
    "\n",
    "            generated_words = [id2word[id] for id in current_sequence\n",
    "                             if id not in [word2id['<START>'], word2id['<END>'], word2id['<PAD>']]]\n",
    "\n",
    "            generated_text = ' '.join(generated_words)\n",
    "            generated_texts.append(generated_text)\n",
    "\n",
    "    return generated_texts\n",
    "\n",
    "# Generate samples with different temperatures\n",
    "print(\"Generating text samples with different temperatures...\")\n",
    "temperatures = [0.5, 0.7, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature: {temp}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    generated_samples = generate_text(\n",
    "        generator=generator,\n",
    "        word2id=word2id,\n",
    "        id2word=id2word,\n",
    "        seq_length=20,\n",
    "        num_samples=3,\n",
    "        temperature=temp,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    for i, text in enumerate(generated_samples, 1):\n",
    "        print(f\"\\nSample {i}:\")\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interactive Text Generation\n",
      "Enter parameters (press Enter for defaults)\n",
      "\n",
      "Generated Text:\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 1:\n",
      "کے کی میں کے <UNK> کے کی <UNK> <UNK> <UNK> <UNK> کی <UNK> <UNK> میں جو <UNK> کی <UNK> <UNK> کے <UNK> <UNK> میں کی میں کی <UNK> کے کے سال کے <UNK> <UNK> کی <UNK> کے کی کے <UNK> کا کے <UNK> <UNK> کی کے کے سے <UNK> <UNK> <UNK> <UNK> کے <UNK> <UNK> <UNK> <UNK> میں کی <UNK> کے <UNK> کے کے کے کی کی میں <UNK> <UNK> کی <UNK> <UNK> <UNK> کی <UNK> کی <UNK> <UNK> <UNK> افراد <UNK> کی <UNK> کی <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> کی کے سے <UNK> <UNK> <UNK> کی میں کے\n",
      "\n",
      "Sample 2:\n",
      "کی میں کی <UNK> <UNK> <UNK> <UNK> کی <UNK> <UNK> <UNK> کی <UNK> <UNK> <UNK> کے کی <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> کے ریٹیل کے کے کی کے <UNK> <UNK> میں <UNK> کے کی کے <UNK> <UNK> کی <UNK> میں کی <UNK> کی <UNK> کے کی <UNK> کی <UNK> میں <UNK> <UNK> <UNK> <UNK> کی کے کی کے <UNK> کی <UNK> <UNK> کے کی کے <UNK> <UNK> میں <UNK> کی سے میں <UNK> <UNK> سی میں <UNK> <UNK> کے <UNK> <UNK> سفیر کے <UNK> کے کی <UNK> کی میں <UNK> کی کے میں <UNK> <UNK> <UNK> سرمایہ <UNK>\n",
      "\n",
      "Sample 3:\n",
      "<UNK> قیمت <UNK> کے کی کے <UNK> <UNK> کی <UNK> کی <UNK> کی جو کی کے <UNK> پر <UNK> <UNK> <UNK> کے کی <UNK> کی <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> قیمت <UNK> کی پر <UNK> <UNK> کی <UNK> کے کی کی <UNK> <UNK> <UNK> میں <UNK> <UNK> <UNK> <UNK> میں <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> بڑھ <UNK> <UNK> کی <UNK> <UNK> <UNK> کی <UNK> کے <UNK> کی کے کے <UNK> کی میں کے <UNK> <UNK> کی <UNK> کی <UNK> کا <UNK> کے <UNK> <UNK> <UNK> نے فیصد سے <UNK> منصوبوں <UNK> کی سروسز کے کے <UNK> <UNK> <UNK>\n"
     ]
    }
   ],
   "source": [
    "def generate_interactive():\n",
    "    print(\"\\nInteractive Text Generation\")\n",
    "    print(\"Enter parameters (press Enter for defaults)\")\n",
    "\n",
    "    try:\n",
    "        seq_length = int(input(\"Sequence length (default: 20): \") or 20)\n",
    "        num_samples = int(input(\"Number of samples (default: 1): \") or 1)\n",
    "        temperature = float(input(\"Temperature (0.1-2.0, default: 1.0): \") or 1.0)\n",
    "\n",
    "        generated_samples = generate_text(\n",
    "            generator=generator,\n",
    "            word2id=word2id,\n",
    "            id2word=id2word,\n",
    "            seq_length=seq_length,\n",
    "            num_samples=num_samples,\n",
    "            temperature=temperature,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        print(\"\\nGenerated Text:\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, text in enumerate(generated_samples, 1):\n",
    "            print(f\"\\nSample {i}:\")\n",
    "            print(text)\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: Invalid input - {e}\")\n",
    "        return\n",
    "\n",
    "# Run interactive generation\n",
    "while True:\n",
    "    choice = input(\"\\nWould you like to generate text? (y/n): \").lower()\n",
    "    if choice != 'y':\n",
    "        break\n",
    "    generate_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
